models:
  - type: main
    engine: nvidia_ai_endpoints
    model: meta/llama-3.1-8b-instruct  #NVIDIA NIM selection
  
  - type: embeddings
    engine: nvidia_ai_endpoints
    model: NV-Embed-QA

instructions:
  - type: general
    content: |
      Below is a conversation between a user and this RAG chatbot based on the uploaded documents.
      The bot is designed to answer questions based on the loaded documents.
      The bot is only knowledgeable about loaded documents.
      If the bot does not know the answer to a question, it truthfully says it does not know.

sample_conversation: |
  user "Hello there!"
    express greeting
  bot express greeting
    "Hello! How can I assist you today?"
  user "Can you help me anser a few questions about the loaded documents?"
    ask for assistance
  bot confirm and offer assistance
    "I'm here to help answer any questions you may have from the loaded content. What would you like to know?"
  user "Can you summarise the loaded docuements?"
    ask about the summary of the documents
  bot respond with a sumary of the documents
    "The loaded documents are about ...."
  user "thanks"
    express appreciation
  bot express appreciation and offer additional help
    "You're welcome. If you have any more questions or if there's anything else I can help you with, please don't hesitate to ask."

rails:
  # Input rails are invoked when new input from the user is received.
  input:
    flows:
      - self check input
      #- self check hallucinations
      

  # Output rails are triggered after a bot message has been generated.
  output:
    flows:
      - self check hallucinations
      - self check output

#actions:
#  type: custom
#  name: rag
#  return_type: ActionResult

  # Whether to try to use a single LLM call for generating the user intent, next step and bot message.
dialog:
  single_call:
    enabled: True
    # If a single call fails, whether to fall back to multiple LLM calls.
    fallback_to_multiple_calls: True

streaming: True
  
#knowledge_bases:
#  - name: my_knowledge_base  # Choose a name
#    type: llama_index  # Or another supported type if you're not using Llama Index
#    path: ./Config/kb  # Path to your knowledge base directory
